services:
  llama_cpp:
    image: ghcr.io/ggerganov/llama.cpp:server-cuda-cu121
    container_name: llama-cpp
    restart: unless-stopped
    network_mode: "container:ai-net"
    command: --model /models/sizin-model-dosyaniz.gguf --host 0.0.0.0 --port 8081 --n-gpu-layers -1
    volumes:
      - model_storage:/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
volumes:
  model_storage: {}
